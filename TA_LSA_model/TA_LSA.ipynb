{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imorting useful libraries for the LSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, MiniBatchKMeans\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading our collected news headlines dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3328, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('C:/Users/Muzammil/Desktop/TA_Assignment_1/TA_Data_Collection/sentences.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3323</th>\n",
       "      <td>India’s forex reserves increase, stand at $562...</td>\n",
       "      <td>MUMBAI: India’s foreign exchange reserves rose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3324</th>\n",
       "      <td>Ford to cut 1,100 jobs in Spain</td>\n",
       "      <td>MADRID: U.S. auto maker Ford plans to slash 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3325</th>\n",
       "      <td>Sri Lankan shares snap 6-day rally as financia...</td>\n",
       "      <td>Sri Lankan shares closed lower on Friday, afte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3326</th>\n",
       "      <td>NY cocoa to fall to $2,692</td>\n",
       "      <td>SINGAPORE: New York May cocoa is expected to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3327</th>\n",
       "      <td>Banks drag FTSE 100 to 1-month low</td>\n",
       "      <td>London’s blue-chip FTSE 100 index fell on Frid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "3323  India’s forex reserves increase, stand at $562...   \n",
       "3324                    Ford to cut 1,100 jobs in Spain   \n",
       "3325  Sri Lankan shares snap 6-day rally as financia...   \n",
       "3326                         NY cocoa to fall to $2,692   \n",
       "3327                 Banks drag FTSE 100 to 1-month low   \n",
       "\n",
       "                                                   Text  \n",
       "3323  MUMBAI: India’s foreign exchange reserves rose...  \n",
       "3324  MADRID: U.S. auto maker Ford plans to slash 1,...  \n",
       "3325  Sri Lankan shares closed lower on Friday, afte...  \n",
       "3326  SINGAPORE: New York May cocoa is expected to b...  \n",
       "3327  London’s blue-chip FTSE 100 index fell on Frid...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating TF-IDF Vectorizer on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "['aa' 'aafrinish' 'aaj' ... 'zte' 'zu' 'zuckerberg']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english',token_pattern=r'(?u)\\b[A-Za-z]+\\b')\n",
    "TF_IDF_Vectorizer = tfidf_vectorizer.fit_transform(data.sentence).toarray()\n",
    "\n",
    "print(TF_IDF_Vectorizer)\n",
    "print(tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Applying truncated SVD on the TF-IDF Vector which is calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3328, 300)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "lsa = TruncatedSVD(n_components=300, n_iter=100)\n",
    "corpus = lsa.fit_transform(TF_IDF_Vectorizer)\n",
    "corpus.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input random text to check out sentence similarities with the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = tfidf_vectorizer.transform(['pemra bans airing of imran khan’s speeches']).toarray()\n",
    "input_corpus = lsa.transform(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(input_corpus).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Cosine similarities between the input text embedding and data sentence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.81338713  0.89931404 ...  0.02027968 -0.03407232\n",
      "  -0.04502325]]\n",
      "1.0000000000000002\n",
      "0\n",
      "1.0000000000000002\n",
      "[  0 140 648 499 462]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cos_similarities = cosine_similarity(np.array(input_corpus) , np.array(corpus))\n",
    "print(cos_similarities)\n",
    "print(cos_similarities.max())\n",
    "print(cos_similarities[0].argmax())\n",
    "print(cos_similarities[0].max())\n",
    "s= cos_similarities[0].argsort()[-5:][::-1]\n",
    "print(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now printing top 5 results which are similar to the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEMRA bans airing of Imran Khan’s speeches\n",
      "PEMRA bans airing of Imran Khan’s speeches\n",
      "PEMRA bans airing of Imran Khan’s speeches: Gagged\n",
      "Pemra bans Imran’s speeches, again\n",
      "Imran Khan moves LHC against Pemra’s ban on broadcast of his speeches\n"
     ]
    }
   ],
   "source": [
    "for i in s:\n",
    "    print(data.sentence[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Mean Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score: 0.16970633377069289\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans.fit(corpus)\n",
    "\n",
    "# calculate the silhouette score to evaluate the quality of clustering\n",
    "silhouette_avg = silhouette_score(corpus, kmeans.labels_)\n",
    "print(f\"Silhouette score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score: 0.5297378545765987\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(corpus)\n",
    "\n",
    "# calculate the silhouette score to evaluate the quality of clustering\n",
    "silhouette_avg = silhouette_score(corpus, kmeans.labels_)\n",
    "print(f\"Silhouette score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score: 0.18454114300211424\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "kmeans.fit(corpus)\n",
    "\n",
    "# calculate the silhouette score to evaluate the quality of clustering\n",
    "silhouette_avg = silhouette_score(corpus, kmeans.labels_)\n",
    "print(f\"Silhouette score: {silhouette_avg}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniBatch K-Mean Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score: 0.15167897129292182\n"
     ]
    }
   ],
   "source": [
    "mb_km = MiniBatchKMeans(n_clusters=5)\n",
    "mb_km.fit(corpus)\n",
    "\n",
    "# calculate the silhouette score to evaluate the quality of clustering\n",
    "silhouette_avg = silhouette_score(corpus, mb_km.labels_)\n",
    "print(f\"Silhouette score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score: 0.19250160636701083\n"
     ]
    }
   ],
   "source": [
    "mb_km = MiniBatchKMeans(n_clusters=3)\n",
    "mb_km.fit(corpus)\n",
    "\n",
    "# calculate the silhouette score to evaluate the quality of clustering\n",
    "silhouette_avg = silhouette_score(corpus, mb_km.labels_)\n",
    "print(f\"Silhouette score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score: 0.16414340383859527\n"
     ]
    }
   ],
   "source": [
    "mb_km = MiniBatchKMeans(n_clusters=4)\n",
    "mb_km.fit(corpus)\n",
    "\n",
    "# calculate the silhouette score to evaluate the quality of clustering\n",
    "silhouette_avg = silhouette_score(corpus, mb_km.labels_)\n",
    "print(f\"Silhouette score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score: 0.1936359678685607\n"
     ]
    }
   ],
   "source": [
    "mb_km = MiniBatchKMeans(n_clusters=5)\n",
    "mb_km.fit(corpus)\n",
    "\n",
    "# calculate the silhouette score to evaluate the quality of clustering\n",
    "silhouette_avg = silhouette_score(corpus, mb_km.labels_)\n",
    "print(f\"Silhouette score: {silhouette_avg}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomarative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score: 0.5368323155738473\n"
     ]
    }
   ],
   "source": [
    "aglo_cluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')\n",
    "aglo_cluster.fit(corpus)\n",
    "\n",
    "# calculate the silhouette score to evaluate the quality of clustering\n",
    "silhouette_avg = silhouette_score(corpus, aglo_cluster.labels_)\n",
    "print(f\"Silhouette score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score: 0.5476116479984486\n"
     ]
    }
   ],
   "source": [
    "aglo_cluster = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')\n",
    "aglo_cluster.fit(corpus)\n",
    "\n",
    "# calculate the silhouette score to evaluate the quality of clustering\n",
    "silhouette_avg = silhouette_score(corpus, aglo_cluster.labels_)\n",
    "print(f\"Silhouette score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score: 0.5412118520484813\n"
     ]
    }
   ],
   "source": [
    "aglo_cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n",
    "aglo_cluster.fit(corpus)\n",
    "\n",
    "# calculate the silhouette score to evaluate the quality of clustering\n",
    "silhouette_avg = silhouette_score(corpus, aglo_cluster.labels_)\n",
    "print(f\"Silhouette score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score: 0.5228862592712163\n"
     ]
    }
   ],
   "source": [
    "aglo_cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\n",
    "aglo_cluster.fit(corpus)\n",
    "\n",
    "# calculate the silhouette score to evaluate the quality of clustering\n",
    "silhouette_avg = silhouette_score(corpus, aglo_cluster.labels_)\n",
    "print(f\"Silhouette score: {silhouette_avg}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
